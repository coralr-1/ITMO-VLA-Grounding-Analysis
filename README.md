# ğŸ¤– Temporal Consistency in Visual Grounding for Vision-Language-Action Models
# ğŸ¤– è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­çš„æ—¶åºä¸€è‡´æ€§è§†è§‰å®šä½ç ”ç©¶

> **Research Proposal for BE2R Lab Admission Test**  
> **BE2Rå®éªŒå®¤æ‹›è˜æµ‹è¯•ç ”ç©¶ææ¡ˆ**
> 
> **ITMO University | Biomechatronics and Energy-Efficient Robotics Laboratory**  
> **ITMOå¤§å­¦ | ç”Ÿç‰©æœºç”µä¸èŠ‚èƒ½æœºå™¨äººå®éªŒå®¤**
> 
> **Submission Deadline**: February 9, 2025  
> **æäº¤æˆªæ­¢æ—¥æœŸ**: 2025å¹´2æœˆ9æ—¥

---

[English](#english) | [ä¸­æ–‡](#chinese)

---

<a name="english"></a>

## ğŸ“– English Version

[![Research Paper](https://img.shields.io/badge/Research_Proposal-PDF-red?style=flat-square)](./ITMO_Lab_Essay.pdf)
[![Demo Video](https://img.shields.io/badge/Demo-ASL_Robot-blue?style=flat-square)](./My_Project_for_IROS_2026/LLM_robots.mp4)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](./LICENSE)

### ğŸ¯ Project Overview

This repository contains a **critical analysis of state-of-the-art Vision-Language-Action (VLA) models** for robotic manipulation, identifying a novel research problem and proposing the **Temporal Grounding Memory (TGM)** framework to address temporal attention instability in long-horizon tasks.

**Research Direction**: VLA for Manipulation (as required by BE2R Lab)

### âœ¨ Key Contributions

| Contribution | Description |
|-------------|-------------|
| ğŸ” **Critical Gap Identified** | Discovered temporal attention inconsistency in AAAI 2026 Best Paper (ReconVLA) |
| ğŸ“Š **Quantitative Evidence** | Documented 31.5% performance degradation on CALVIN 5-task chains |
| ğŸ’¡ **Novel Solution** | Proposed TGM framework with projected 10-15% improvement |
| ğŸ§  **Systematic Analysis** | Complete problem â†’ hypothesis â†’ method chain with mathematical formalization |
| ğŸ¤– **Practical Implementation** | Demonstrated understanding via code reproduction and robotics project |

---

### ğŸ“‚ Repository Structure

```
ITMO-VLA-Grounding-Analysis/
â”‚
â”œâ”€â”€ ğŸ“„ ITMO_Lab_Essay.pdf                    # Main research proposal (LaTeX-generated)
â”œâ”€â”€ ğŸ“„ README.md                              # This file
â”‚
â”œâ”€â”€ ğŸ“ papers/                                # Analyzed research papers
â”‚   â”œâ”€â”€ ReconVLA_AAAI2026.pdf                # â­ AAAI 2026 Best Paper
â”‚   â”œâ”€â”€ RSS_Stable_Language_Guidance_2026.pdf
â”‚   â”œâ”€â”€ SpatialVLA_2025.pdf
â”‚   â””â”€â”€ README.md                            # Paper analysis summary
â”‚
â”œâ”€â”€ ğŸ“ reproduced_code/                       # Code reproduction & experiments
â”‚   â”œâ”€â”€ ReconVLA_Reproduction.ipynb          # ReconVLA baseline reproduction
â”‚   â”œâ”€â”€ task_animation.mp4                   # Experiment visualization
â”‚   â””â”€â”€ README.md                            # Reproduction documentation
â”‚
â””â”€â”€ ğŸ“ My_Project_for_IROS_2026/             # Practical robotics implementation
    â”œâ”€â”€ LLM_robots.mp4                       # Demo video
    â”œâ”€â”€ README.MD                            # Project overview
    â””â”€â”€ asl_robot_project/                   # AI-powered Shadow Hand ASL control
        â”œâ”€â”€ asl_main.py                      # Main control script
        â”œâ”€â”€ my_shadow_hand.urdf              # Robot model
        â”œâ”€â”€ requirements.txt                 # Dependencies
        â””â”€â”€ README.md                        # Detailed documentation
```

---

### ğŸ”¬ Research Problem Statement

#### Background

Recent VLA models (RT-2, OpenVLA, ReconVLA) have achieved remarkable progress in robotic manipulation by integrating vision-language models with action prediction. However, our analysis reveals a critical limitation:

**Problem**: Current VLA models suffer from **temporal attention instability** during long-horizon tasks, where frame-wise visual grounding lacks temporal coherence.

#### Evidence from ReconVLA (AAAI 2026 Best Paper)

| Task Complexity | Success Rate | Degradation |
|----------------|--------------|-------------|
| 1 subtask | 95.6% | Baseline |
| 2 subtasks | 87.6% | -8.0% |
| 3 subtasks | 76.9% | -18.7% |
| **5 subtasks** | **64.1%** | **-31.5%** |

**Analysis**: The accelerating degradation pattern (not linear) indicates a systematic issue beyond task difficulty accumulation.

#### Root Cause

ReconVLA's reconstruction operates **frame-independently**:
- Each frame's gaze region is predicted without considering previous frames
- No temporal smoothness constraints
- No subtask boundary awareness

This leads to **attention jumps** where the robot's focus switches unpredictably between objects, causing:
- Grasping wrong objects
- Premature attention switching before subtask completion
- Increased failure rate in multi-step manipulation sequences

---

### ğŸ’¡ Proposed Solution: Temporal Grounding Memory (TGM)

#### Core Innovation

Augment ReconVLA with a **temporal memory module** that maintains coherent attention across frames while allowing intelligent switching at subtask boundaries.

#### Mathematical Formulation

**1. Temporal Memory State**
```
M_t = {g_{t-Ï„}, ..., g_{t-1}}  # Past Ï„ frames' gaze features
```

**2. Attention Fusion**
```
h_temporal^t = Attention(h_visual^t, M_t)
h_final^t = h_visual^t + Î± Â· h_temporal^t
```

**3. Temporal Smoothness Loss**
```
L_smooth = ||g_t - g_{t-1}||Â² Â· (1 - s_t)
```
where `s_t` is the subtask switching flag:
- `s_t = 0`: Within subtask â†’ penalize large jumps
- `s_t = 1`: At subtask boundary â†’ allow switching

**4. Overall Loss**
```
L_TGM = L_action + L_recon + Î»_smooth Â· L_smooth
```

#### Expected Results

| Method | 5-task Success | Improvement |
|--------|---------------|-------------|
| ReconVLA (Baseline) | 64.1% | - |
| **TGM-Full (Ours)** | **76.0%** | **+11.9%** |

---

### ğŸ“š Analyzed Papers

We selected three papers forming a complete technical evolution chain:

#### 1. RSS: Residual Semantic Steering (arXiv 2026)
- **Focus**: Language robustness via Monte Carlo Syntactic Integration
- **Gap**: Does not address visual attention mechanisms
- **Key Metric**: 82.2% success on M8 corrupted instructions

#### 2. SpatialVLA (arXiv 2025)
- **Focus**: 3D spatial representations with Ego3D Position Encoding
- **Gap**: Lacks temporal information modeling
- **Key Metric**: 88.2% on spatial reasoning tasks

#### 3. â­ ReconVLA (AAAI 2026 Best Paper)
- **Focus**: Implicit visual grounding via gaze region reconstruction
- **Gap**: Frame-independent reconstruction â†’ temporal inconsistency
- **Key Metric**: 64.1% on CALVIN 5-task chains (31.5% degradation from single-task)

**Comparative Analysis Table**:

| Capability | RSS | SpatialVLA | ReconVLA | **TGM (Proposed)** |
|-----------|-----|------------|----------|-------------------|
| Language Robustness | âœ“âœ“ | âœ“ | âœ“ | âœ“ |
| Spatial Understanding | âœ— | âœ“âœ“ | âœ“ | âœ“ |
| Visual Grounding | âœ— | âœ“ | âœ“âœ“ | âœ“âœ“ |
| **Temporal Consistency** | âœ— | âœ— | âœ— | **âœ“âœ“** |

ğŸ“– [Detailed Paper Analysis](./papers/README.md)

---

### ğŸ§ª Code Reproduction

To validate our understanding of the problem, we reproduced ReconVLA's pipeline and analyzed attention patterns in failure cases.

**Reproduced Components**:
- âœ… Visual feature extraction (SigLIP encoder)
- âœ… Reconstruction module (Diffusion Transformer)
- âœ… CALVIN evaluation protocol
- âœ… Attention visualization tools

**Key Findings**:
- Confirmed attention jump phenomenon in sequential tasks
- Observed instability increases with task chain length
- Identified specific failure patterns in "stack block" scenarios

ğŸ”§ [View Reproduction Code & Results](./reproduced_code/README.md)

---

### ğŸ¤– Practical Implementation: AI-Powered ASL Robot

To demonstrate practical robotics capabilities and understanding of vision-language-action integration, we developed an **intelligent Shadow Hand control system** for American Sign Language.

**System Features**:
- ğŸ¤ Voice recognition (Chinese input)
- ğŸ§  LLM-based error correction (fixes ASR mistakes)
- ğŸŒ Translation (Chinese â†’ English)
- ğŸ¤– Closed-loop control (smooth ASL gestures A-Z)

**Technical Highlights**:
- PyBullet physics simulation
- Real-time joint position feedback
- Local LLM integration (LM Studio)
- Complete ASL alphabet implementation

ğŸ¥ [Watch Demo Video](./My_Project_for_IROS_2026/LLM_robots.mp4)  
ğŸ“– [Detailed ASL Project Documentation](./My_Project_for_IROS_2026/asl_robot_project/README.md)

---

### ğŸ“Š Methodology & Validation Plan

#### Datasets
- **Primary**: CALVIN Benchmark (ABCâ†’D split)
- **Supplementary**: LIBERO-Long (extreme long-horizon scenarios)

#### Baselines
1. ReconVLA (AAAI 2026) - Primary baseline
2. ReconVLA + Simple Smoothing - Validates problem existence
3. TGM-NoSwitch - Ablation without subtask detection
4. TGM-Full - Complete proposed method

#### Evaluation Metrics
- Success rate on 1/5 through 5/5 task chains
- Average completion length
- Attention stability (via visualization)
- Computational overhead

#### Hypothesis Validation

**H1** (Temporal Memory Effectiveness):
```
TGM-NoSwitch vs ReconVLA â†’ Expected: >5% improvement
```

**H2** (Subtask Detection Necessity):
```
TGM-Full vs TGM-NoSwitch â†’ Expected: >3% improvement
```

**H3** (Smoothness Loss Contribution):
```
Ablate L_smooth â†’ Evaluate constraint impact
```

---

### ğŸ† Why This Research Matters

#### Scientific Contributions
1. **First systematic identification** of temporal attention inconsistency in VLA models
2. **Quantitative evidence** of the problem's severity (31.5% degradation)
3. **Principled solution** with mathematical formalization
4. **Extends AAAI Best Paper** in a complementary (not competitive) direction

#### Practical Impact
- Enables longer manipulation sequences (10+ steps)
- Reduces failure rate by ~12% (critical for deployment)
- Maintains computational efficiency (+15% overhead for +12% performance)
- Opens new research direction: temporal consistency in foundation models

#### Alignment with BE2R Lab Directions
- âœ… VLA for manipulation
- âœ… Long-horizon tasks
- âœ… Robust to OOD scenarios (attention stability)
- âœ… Embodiment-agnostic (works with any VLA backbone)

---

### ğŸ“… Implementation Timeline

**Phase 1: Problem Validation** (1 week)
- Reproduce ReconVLA's CALVIN results
- Visualize attention jump patterns
- Statistical analysis of failure modes

**Phase 2: Prototype Development** (2 weeks)
- Implement Temporal Memory module
- Integrate with ReconVLA architecture
- Initial feasibility tests

**Phase 3: Full Training** (1 week)
- Train TGM-Full on CALVIN
- Hyperparameter tuning (Ï„, Î»_smooth, Î±)

**Phase 4: Evaluation** (1 week)
- Comprehensive experiments
- Ablation studies
- Visualization & analysis

**Total**: ~5 weeks

---

### ğŸ’» Quick Start

#### Prerequisites
- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ (for training)

#### Installation

```bash
# Clone repository
git clone https://github.com/[your-username]/ITMO-VLA-Grounding-Analysis.git
cd ITMO-VLA-Grounding-Analysis

# Install dependencies for code reproduction
cd reproduced_code
pip install -r requirements.txt

# Install dependencies for ASL robot project
cd ../My_Project_for_IROS_2026/asl_robot_project
pip install -r requirements.txt
```

#### Running Reproduced Code

```bash
cd reproduced_code
jupyter notebook ReconVLA_Reproduction.ipynb
```

#### Running ASL Robot Demo

```bash
cd My_Project_for_IROS_2026/asl_robot_project
python asl_main.py
```

---

### ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| [ITMO_Lab_Essay.pdf](./ITMO_Lab_Essay.pdf) | Complete research proposal (LaTeX) |
| [papers/README.md](./papers/README.md) | Detailed paper analysis |
| [reproduced_code/README.md](./reproduced_code/README.md) | Reproduction documentation |
| [My_Project_for_IROS_2026/asl_robot_project/README.md](./My_Project_for_IROS_2026/asl_robot_project/README.md) | ASL robot system guide |

---

### ğŸ¤ Acknowledgments

- **Papers Analyzed**: RSS (Zhan et al.), SpatialVLA (Qu et al.), ReconVLA (Song et al.)
- **Benchmarks**: CALVIN (Mees et al.), LIBERO
- **Models**: OpenVLA, RT-2
- **Tools**: PyBullet, LM Studio, PyTorch

---

### ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](./LICENSE) file for details.

Research proposal submitted for educational/admission purposes to ITMO University BE2R Lab.

---

<a name="chinese"></a>

## ğŸ“– ä¸­æ–‡ç‰ˆæœ¬

[![ç ”ç©¶è®ºæ–‡](https://img.shields.io/badge/ç ”ç©¶ææ¡ˆ-PDF-red?style=flat-square)](./ITMO_Lab_Essay.pdf)
[![æ¼”ç¤ºè§†é¢‘](https://img.shields.io/badge/æ¼”ç¤º-ASLæœºå™¨äºº-blue?style=flat-square)](./My_Project_for_IROS_2026/LLM_robots.mp4)
[![è®¸å¯è¯](https://img.shields.io/badge/è®¸å¯è¯-MIT-green?style=flat-square)](./LICENSE)

### ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬ä»“åº“åŒ…å«å¯¹**æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹**çš„æ‰¹åˆ¤æ€§åˆ†æï¼Œè¯†åˆ«å‡ºä¸€ä¸ªæ–°çš„ç ”ç©¶é—®é¢˜ï¼Œå¹¶æå‡º**æ—¶åºå®šä½è®°å¿†(TGM)æ¡†æ¶**æ¥è§£å†³é•¿æ—¶åºä»»åŠ¡ä¸­çš„æ—¶åºæ³¨æ„åŠ›ä¸ç¨³å®šé—®é¢˜ã€‚

**ç ”ç©¶æ–¹å‘**: VLA for Manipulationï¼ˆç¬¦åˆBE2Rå®éªŒå®¤è¦æ±‚ï¼‰

### âœ¨ æ ¸å¿ƒè´¡çŒ®

| è´¡çŒ®ç‚¹ | è¯´æ˜ |
|-------|------|
| ğŸ” **è¯†åˆ«å…³é”®ç¼ºé™·** | å‘ç°AAAI 2026æœ€ä½³è®ºæ–‡(ReconVLA)ä¸­çš„æ—¶åºæ³¨æ„åŠ›ä¸ä¸€è‡´é—®é¢˜ |
| ğŸ“Š **å®šé‡è¯æ®** | è®°å½•äº†CALVIN 5ä»»åŠ¡é“¾ä¸Š31.5%çš„æ€§èƒ½ä¸‹é™ |
| ğŸ’¡ **åˆ›æ–°è§£å†³æ–¹æ¡ˆ** | æå‡ºTGMæ¡†æ¶ï¼Œé¢„æœŸæå‡10-15%æ€§èƒ½ |
| ğŸ§  **ç³»ç»Ÿæ€§åˆ†æ** | å®Œæ•´çš„é—®é¢˜â†’å‡è®¾â†’æ–¹æ³•é“¾ï¼Œå¸¦æ•°å­¦å½¢å¼åŒ– |
| ğŸ¤– **å®è·µå®ç°** | é€šè¿‡ä»£ç å¤ç°å’Œæœºå™¨äººé¡¹ç›®å±•ç¤ºç†è§£æ·±åº¦ |

---

### ğŸ“‚ ä»“åº“ç»“æ„

```
ITMO-VLA-Grounding-Analysis/
â”‚
â”œâ”€â”€ ğŸ“„ ITMO_Lab_Essay.pdf                    # ä¸»ç ”ç©¶ææ¡ˆï¼ˆLaTeXç”Ÿæˆï¼‰
â”œâ”€â”€ ğŸ“„ README.md                              # æœ¬æ–‡ä»¶
â”‚
â”œâ”€â”€ ğŸ“ papers/                                # åˆ†æçš„ç ”ç©¶è®ºæ–‡
â”‚   â”œâ”€â”€ ReconVLA_AAAI2026.pdf                # â­ AAAI 2026æœ€ä½³è®ºæ–‡
â”‚   â”œâ”€â”€ RSS_Stable_Language_Guidance_2026.pdf
â”‚   â”œâ”€â”€ SpatialVLA_2025.pdf
â”‚   â””â”€â”€ README.md                            # è®ºæ–‡åˆ†ææ€»ç»“
â”‚
â”œâ”€â”€ ğŸ“ reproduced_code/                       # ä»£ç å¤ç°ä¸å®éªŒ
â”‚   â”œâ”€â”€ ReconVLA_Reproduction.ipynb          # ReconVLAåŸºçº¿å¤ç°
â”‚   â”œâ”€â”€ task_animation.mp4                   # å®éªŒå¯è§†åŒ–
â”‚   â””â”€â”€ README.md                            # å¤ç°æ–‡æ¡£
â”‚
â””â”€â”€ ğŸ“ My_Project_for_IROS_2026/             # å®è·µæœºå™¨äººå®ç°
    â”œâ”€â”€ LLM_robots.mp4                       # æ¼”ç¤ºè§†é¢‘
    â”œâ”€â”€ README.MD                            # é¡¹ç›®æ¦‚è¿°
    â””â”€â”€ asl_robot_project/                   # AIé©±åŠ¨çš„Shadow Handæ‰‹è¯­æ§åˆ¶
        â”œâ”€â”€ asl_main.py                      # ä¸»æ§åˆ¶è„šæœ¬
        â”œâ”€â”€ my_shadow_hand.urdf              # æœºå™¨äººæ¨¡å‹
        â”œâ”€â”€ requirements.txt                 # ä¾èµ–é¡¹
        â””â”€â”€ README.md                        # è¯¦ç»†æ–‡æ¡£
```

---

### ğŸ”¬ ç ”ç©¶é—®é¢˜é™ˆè¿°

#### èƒŒæ™¯

è¿‘æœŸçš„VLAæ¨¡å‹ï¼ˆRT-2, OpenVLA, ReconVLAï¼‰é€šè¿‡æ•´åˆè§†è§‰-è¯­è¨€æ¨¡å‹ä¸åŠ¨ä½œé¢„æµ‹åœ¨æœºå™¨äººæ“ä½œä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®å±€é™ï¼š

**é—®é¢˜**: å½“å‰VLAæ¨¡å‹åœ¨é•¿æ—¶åºä»»åŠ¡ä¸­å­˜åœ¨**æ—¶åºæ³¨æ„åŠ›ä¸ç¨³å®š**ï¼Œå…¶ä¸­é€å¸§è§†è§‰å®šä½ç¼ºä¹æ—¶åºè¿è´¯æ€§ã€‚

#### æ¥è‡ªReconVLAçš„è¯æ®ï¼ˆAAAI 2026æœ€ä½³è®ºæ–‡ï¼‰

| ä»»åŠ¡å¤æ‚åº¦ | æˆåŠŸç‡ | æ€§èƒ½ä¸‹é™ |
|-----------|--------|---------|
| 1ä¸ªå­ä»»åŠ¡ | 95.6% | åŸºå‡† |
| 2ä¸ªå­ä»»åŠ¡ | 87.6% | -8.0% |
| 3ä¸ªå­ä»»åŠ¡ | 76.9% | -18.7% |
| **5ä¸ªå­ä»»åŠ¡** | **64.1%** | **-31.5%** |

**åˆ†æ**: åŠ é€Ÿä¸‹é™æ¨¡å¼ï¼ˆéçº¿æ€§ï¼‰è¡¨æ˜å­˜åœ¨è¶…è¶Šä»»åŠ¡éš¾åº¦ç´¯ç§¯çš„ç³»ç»Ÿæ€§é—®é¢˜ã€‚

#### æ ¹æœ¬åŸå› 

ReconVLAçš„é‡å»ºè¿‡ç¨‹**é€å¸§ç‹¬ç«‹**è¿è¡Œï¼š
- æ¯å¸§çš„gaze regioné¢„æµ‹ä¸è€ƒè™‘å‰åºå¸§
- æ²¡æœ‰æ—¶åºå¹³æ»‘æ€§çº¦æŸ
- æ²¡æœ‰å­ä»»åŠ¡è¾¹ç•Œæ„ŸçŸ¥

è¿™å¯¼è‡´**æ³¨æ„åŠ›è·³å˜**ï¼Œæœºå™¨äººçš„ç„¦ç‚¹åœ¨ç‰©ä½“é—´ä¸å¯é¢„æµ‹åœ°åˆ‡æ¢ï¼Œé€ æˆï¼š
- æŠ“å–é”™è¯¯çš„ç‰©ä½“
- å­ä»»åŠ¡å®Œæˆå‰è¿‡æ—©åˆ‡æ¢æ³¨æ„åŠ›
- å¤šæ­¥æ“ä½œåºåˆ—å¤±è´¥ç‡å¢åŠ 

---

### ğŸ’¡ æå‡ºçš„è§£å†³æ–¹æ¡ˆï¼šæ—¶åºå®šä½è®°å¿†(TGM)

#### æ ¸å¿ƒåˆ›æ–°

åœ¨ReconVLAåŸºç¡€ä¸Šå¢åŠ **æ—¶åºè®°å¿†æ¨¡å—**ï¼Œåœ¨å¸§é—´ä¿æŒè¿è´¯æ³¨æ„åŠ›çš„åŒæ—¶å…è®¸åœ¨å­ä»»åŠ¡è¾¹ç•Œæ™ºèƒ½åˆ‡æ¢ã€‚

#### æ•°å­¦å½¢å¼åŒ–

**1. æ—¶åºè®°å¿†çŠ¶æ€**
```
M_t = {g_{t-Ï„}, ..., g_{t-1}}  # è¿‡å»Ï„å¸§çš„gazeç‰¹å¾
```

**2. æ³¨æ„åŠ›èåˆ**
```
h_temporal^t = Attention(h_visual^t, M_t)
h_final^t = h_visual^t + Î± Â· h_temporal^t
```

**3. æ—¶åºå¹³æ»‘æ€§æŸå¤±**
```
L_smooth = ||g_t - g_{t-1}||Â² Â· (1 - s_t)
```
å…¶ä¸­ `s_t` æ˜¯å­ä»»åŠ¡åˆ‡æ¢æ ‡å¿—ï¼š
- `s_t = 0`: å­ä»»åŠ¡å†… â†’ æƒ©ç½šå¤§è·³å˜
- `s_t = 1`: å­ä»»åŠ¡è¾¹ç•Œ â†’ å…è®¸åˆ‡æ¢

**4. æ€»ä½“æŸå¤±**
```
L_TGM = L_action + L_recon + Î»_smooth Â· L_smooth
```

#### é¢„æœŸç»“æœ

| æ–¹æ³• | 5ä»»åŠ¡æˆåŠŸç‡ | æå‡ |
|-----|-----------|------|
| ReconVLAï¼ˆåŸºçº¿ï¼‰ | 64.1% | - |
| **TGM-Fullï¼ˆæˆ‘ä»¬çš„ï¼‰** | **76.0%** | **+11.9%** |

---

### ğŸ“š åˆ†æçš„è®ºæ–‡

æˆ‘ä»¬é€‰æ‹©äº†ä¸‰ç¯‡å½¢æˆå®Œæ•´æŠ€æœ¯æ¼”è¿›é“¾çš„è®ºæ–‡ï¼š

#### 1. RSS: æ®‹å·®è¯­ä¹‰å¼•å¯¼ï¼ˆarXiv 2026ï¼‰
- **ç„¦ç‚¹**: é€šè¿‡è’™ç‰¹å¡æ´›å¥æ³•é›†æˆå®ç°è¯­è¨€é²æ£’æ€§
- **ç¼ºé™·**: æœªå…³æ³¨è§†è§‰æ³¨æ„åŠ›æœºåˆ¶
- **å…³é”®æŒ‡æ ‡**: M8æŸåæŒ‡ä»¤ä¸Š82.2%æˆåŠŸç‡

#### 2. SpatialVLAï¼ˆarXiv 2025ï¼‰
- **ç„¦ç‚¹**: å¸¦Ego3Dä½ç½®ç¼–ç çš„3Dç©ºé—´è¡¨ç¤º
- **ç¼ºé™·**: ç¼ºä¹æ—¶åºä¿¡æ¯å»ºæ¨¡
- **å…³é”®æŒ‡æ ‡**: ç©ºé—´æ¨ç†ä»»åŠ¡88.2%

#### 3. â­ ReconVLAï¼ˆAAAI 2026æœ€ä½³è®ºæ–‡ï¼‰
- **ç„¦ç‚¹**: é€šè¿‡gazeåŒºåŸŸé‡å»ºå®ç°éšå¼è§†è§‰å®šä½
- **ç¼ºé™·**: é€å¸§ç‹¬ç«‹é‡å»º â†’ æ—¶åºä¸ä¸€è‡´
- **å…³é”®æŒ‡æ ‡**: CALVIN 5ä»»åŠ¡é“¾64.1%ï¼ˆè¾ƒå•ä»»åŠ¡ä¸‹é™31.5%ï¼‰

**å¯¹æ¯”åˆ†æè¡¨**:

| èƒ½åŠ› | RSS | SpatialVLA | ReconVLA | **TGMï¼ˆæå‡ºçš„ï¼‰** |
|-----|-----|------------|----------|-----------------|
| è¯­è¨€é²æ£’æ€§ | âœ“âœ“ | âœ“ | âœ“ | âœ“ |
| ç©ºé—´ç†è§£ | âœ— | âœ“âœ“ | âœ“ | âœ“ |
| è§†è§‰å®šä½ | âœ— | âœ“ | âœ“âœ“ | âœ“âœ“ |
| **æ—¶åºä¸€è‡´æ€§** | âœ— | âœ— | âœ— | **âœ“âœ“** |

ğŸ“– [è¯¦ç»†è®ºæ–‡åˆ†æ](./papers/README.md)

---

### ğŸ§ª ä»£ç å¤ç°

ä¸ºéªŒè¯æˆ‘ä»¬å¯¹é—®é¢˜çš„ç†è§£ï¼Œæˆ‘ä»¬å¤ç°äº†ReconVLAçš„æµç¨‹å¹¶åˆ†æäº†å¤±è´¥æ¡ˆä¾‹ä¸­çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚

**å¤ç°ç»„ä»¶**:
- âœ… è§†è§‰ç‰¹å¾æå–ï¼ˆSigLIPç¼–ç å™¨ï¼‰
- âœ… é‡å»ºæ¨¡å—ï¼ˆDiffusion Transformerï¼‰
- âœ… CALVINè¯„ä¼°åè®®
- âœ… æ³¨æ„åŠ›å¯è§†åŒ–å·¥å…·

**å…³é”®å‘ç°**:
- ç¡®è®¤äº†é¡ºåºä»»åŠ¡ä¸­çš„æ³¨æ„åŠ›è·³å˜ç°è±¡
- è§‚å¯Ÿåˆ°ä¸ç¨³å®šæ€§éšä»»åŠ¡é“¾é•¿åº¦å¢åŠ 
- è¯†åˆ«äº†"stack block"åœºæ™¯ä¸­çš„ç‰¹å®šå¤±è´¥æ¨¡å¼

ğŸ”§ [æŸ¥çœ‹å¤ç°ä»£ç ä¸ç»“æœ](./reproduced_code/README.md)

---

### ğŸ¤– å®è·µå®ç°ï¼šAIé©±åŠ¨çš„ASLæœºå™¨äºº

ä¸ºå±•ç¤ºå®è·µæœºå™¨äººèƒ½åŠ›å’Œå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œé›†æˆçš„ç†è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª**æ™ºèƒ½Shadow Handæ§åˆ¶ç³»ç»Ÿ**ç”¨äºç¾å¼æ‰‹è¯­ã€‚

**ç³»ç»Ÿç‰¹æ€§**:
- ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆä¸­æ–‡è¾“å…¥ï¼‰
- ğŸ§  åŸºäºLLMçš„é”™è¯¯çº æ­£ï¼ˆä¿®å¤ASRé”™è¯¯ï¼‰
- ğŸŒ ç¿»è¯‘ï¼ˆä¸­æ–‡â†’è‹±æ–‡ï¼‰
- ğŸ¤– é—­ç¯æ§åˆ¶ï¼ˆæµç•…çš„ASLæ‰‹åŠ¿A-Zï¼‰

**æŠ€æœ¯äº®ç‚¹**:
- PyBulletç‰©ç†ä»¿çœŸ
- å®æ—¶å…³èŠ‚ä½ç½®åé¦ˆ
- æœ¬åœ°LLMé›†æˆï¼ˆLM Studioï¼‰
- å®Œæ•´ASLå­—æ¯è¡¨å®ç°

ğŸ¥ [è§‚çœ‹æ¼”ç¤ºè§†é¢‘](./My_Project_for_IROS_2026/LLM_robots.mp4)  
ğŸ“– [è¯¦ç»†ASLé¡¹ç›®æ–‡æ¡£](./My_Project_for_IROS_2026/asl_robot_project/README.md)

---

### ğŸ“Š æ–¹æ³•è®ºä¸éªŒè¯è®¡åˆ’

#### æ•°æ®é›†
- **ä¸»è¦**: CALVINåŸºå‡†ï¼ˆABCâ†’Dåˆ†å‰²ï¼‰
- **è¡¥å……**: LIBERO-Longï¼ˆæç«¯é•¿æ—¶åºåœºæ™¯ï¼‰

#### åŸºçº¿
1. ReconVLAï¼ˆAAAI 2026ï¼‰- ä¸»è¦åŸºçº¿
2. ReconVLA + ç®€å•å¹³æ»‘ - éªŒè¯é—®é¢˜å­˜åœ¨æ€§
3. TGM-NoSwitch - ä¸å«å­ä»»åŠ¡æ£€æµ‹çš„æ¶ˆè
4. TGM-Full - å®Œæ•´æå‡ºæ–¹æ³•

#### è¯„ä¼°æŒ‡æ ‡
- 1/5è‡³5/5ä»»åŠ¡é“¾æˆåŠŸç‡
- å¹³å‡å®Œæˆé•¿åº¦
- æ³¨æ„åŠ›ç¨³å®šæ€§ï¼ˆé€šè¿‡å¯è§†åŒ–ï¼‰
- è®¡ç®—å¼€é”€

#### å‡è®¾éªŒè¯

**H1**ï¼ˆæ—¶åºè®°å¿†æœ‰æ•ˆæ€§ï¼‰:
```
TGM-NoSwitch vs ReconVLA â†’ é¢„æœŸ: >5%æå‡
```

**H2**ï¼ˆå­ä»»åŠ¡æ£€æµ‹å¿…è¦æ€§ï¼‰:
```
TGM-Full vs TGM-NoSwitch â†’ é¢„æœŸ: >3%æå‡
```

**H3**ï¼ˆå¹³æ»‘æ€§æŸå¤±è´¡çŒ®ï¼‰:
```
æ¶ˆèL_smooth â†’ è¯„ä¼°çº¦æŸå½±å“
```

---

### ğŸ† ç ”ç©¶æ„ä¹‰

#### ç§‘å­¦è´¡çŒ®
1. **é¦–æ¬¡ç³»ç»Ÿæ€§è¯†åˆ«**VLAæ¨¡å‹ä¸­çš„æ—¶åºæ³¨æ„åŠ›ä¸ä¸€è‡´
2. **å®šé‡è¯æ®**è¯æ˜é—®é¢˜ä¸¥é‡æ€§ï¼ˆ31.5%ä¸‹é™ï¼‰
3. **åŸç†æ€§è§£å†³æ–¹æ¡ˆ**å¸¦æ•°å­¦å½¢å¼åŒ–
4. **å»¶ä¼¸AAAIæœ€ä½³è®ºæ–‡**ï¼Œæ–¹å‘äº’è¡¥ï¼ˆéç«äº‰ï¼‰

#### å®é™…å½±å“
- æ”¯æŒæ›´é•¿æ“ä½œåºåˆ—ï¼ˆ10+æ­¥éª¤ï¼‰
- é™ä½å¤±è´¥ç‡çº¦12%ï¼ˆå¯¹éƒ¨ç½²è‡³å…³é‡è¦ï¼‰
- ä¿æŒè®¡ç®—æ•ˆç‡ï¼ˆ+15%å¼€é”€æ¢å–+12%æ€§èƒ½ï¼‰
- å¼€è¾Ÿæ–°ç ”ç©¶æ–¹å‘ï¼šåŸºç¡€æ¨¡å‹ä¸­çš„æ—¶åºä¸€è‡´æ€§

#### ä¸BE2Rå®éªŒå®¤æ–¹å‘çš„å¥‘åˆ
- âœ… VLA for manipulation
- âœ… é•¿æ—¶åºä»»åŠ¡
- âœ… å¯¹OODåœºæ™¯çš„é²æ£’æ€§ï¼ˆæ³¨æ„åŠ›ç¨³å®šæ€§ï¼‰
- âœ… Embodiment-agnosticï¼ˆé€‚ç”¨äºä»»ä½•VLAéª¨å¹²ï¼‰

---

### ğŸ“… å®æ–½æ—¶é—´çº¿

**é˜¶æ®µ1ï¼šé—®é¢˜éªŒè¯**ï¼ˆ1å‘¨ï¼‰
- å¤ç°ReconVLAçš„CALVINç»“æœ
- å¯è§†åŒ–æ³¨æ„åŠ›è·³å˜æ¨¡å¼
- å¤±è´¥æ¨¡å¼ç»Ÿè®¡åˆ†æ

**é˜¶æ®µ2ï¼šåŸå‹å¼€å‘**ï¼ˆ2å‘¨ï¼‰
- å®ç°æ—¶åºè®°å¿†æ¨¡å—
- é›†æˆåˆ°ReconVLAæ¶æ„
- åˆæ­¥å¯è¡Œæ€§æµ‹è¯•

**é˜¶æ®µ3ï¼šå®Œæ•´è®­ç»ƒ**ï¼ˆ1å‘¨ï¼‰
- åœ¨CALVINä¸Šè®­ç»ƒTGM-Full
- è¶…å‚æ•°è°ƒä¼˜ï¼ˆÏ„, Î»_smooth, Î±ï¼‰

**é˜¶æ®µ4ï¼šè¯„ä¼°**ï¼ˆ1å‘¨ï¼‰
- å…¨é¢å®éªŒ
- æ¶ˆèç ”ç©¶
- å¯è§†åŒ–ä¸åˆ†æ

**æ€»è®¡**: ~5å‘¨

---

### ğŸ’» å¿«é€Ÿå¼€å§‹

#### ç¯å¢ƒè¦æ±‚
- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ï¼ˆç”¨äºè®­ç»ƒï¼‰

#### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/[your-username]/ITMO-VLA-Grounding-Analysis.git
cd ITMO-VLA-Grounding-Analysis

# å®‰è£…ä»£ç å¤ç°ä¾èµ–
cd reproduced_code
pip install -r requirements.txt

# å®‰è£…ASLæœºå™¨äººé¡¹ç›®ä¾èµ–
cd ../My_Project_for_IROS_2026/asl_robot_project
pip install -r requirements.txt
```

#### è¿è¡Œå¤ç°ä»£ç 

```bash
cd reproduced_code
jupyter notebook ReconVLA_Reproduction.ipynb
```

#### è¿è¡ŒASLæœºå™¨äººæ¼”ç¤º

```bash
cd My_Project_for_IROS_2026/asl_robot_project
python asl_main.py
```

---

### ğŸ“– æ–‡æ¡£

| æ–‡æ¡£ | è¯´æ˜ |
|------|-----|
| [ITMO_Lab_Essay.pdf](./ITMO_Lab_Essay.pdf) | å®Œæ•´ç ”ç©¶ææ¡ˆï¼ˆLaTeXï¼‰ |
| [papers/README.md](./papers/README.md) | è¯¦ç»†è®ºæ–‡åˆ†æ |
| [reproduced_code/README.md](./reproduced_code/README.md) | å¤ç°æ–‡æ¡£ |
| [My_Project_for_IROS_2026/asl_robot_project/README.md](./My_Project_for_IROS_2026/asl_robot_project/README.md) | ASLæœºå™¨äººç³»ç»ŸæŒ‡å— |

---

### ğŸ¤ è‡´è°¢

- **åˆ†æè®ºæ–‡**: RSSï¼ˆZhanç­‰ï¼‰ã€SpatialVLAï¼ˆQuç­‰ï¼‰ã€ReconVLAï¼ˆSongç­‰ï¼‰
- **åŸºå‡†æµ‹è¯•**: CALVINï¼ˆMeesç­‰ï¼‰ã€LIBERO
- **æ¨¡å‹**: OpenVLAã€RT-2
- **å·¥å…·**: PyBulletã€LM Studioã€PyTorch

---


---

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§[LICENSE](./LICENSE)æ–‡ä»¶ã€‚

ç ”ç©¶ææ¡ˆæäº¤ç”¨äºITMOå¤§å­¦BE2Rå®éªŒå®¤æ•™è‚²/æ‹›è˜ç›®çš„ã€‚

---

## ğŸŒŸ Star History

If you find this research interesting, please consider giving it a star! â­

å¦‚æœæ‚¨è§‰å¾—è¿™é¡¹ç ”ç©¶æœ‰è¶£ï¼Œè¯·è€ƒè™‘ç»™ä¸ªæ˜Ÿæ ‡ï¼â­

[![Star History Chart](https://api.star-history.com/svg?repos=[your-username]/ITMO-VLA-Grounding-Analysis&type=Date)](https://star-history.com/#[your-username]/ITMO-VLA-Grounding-Analysis&Date)
