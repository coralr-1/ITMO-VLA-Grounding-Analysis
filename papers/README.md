# ğŸ“š Analyzed Papers / åˆ†æè®ºæ–‡

[English](#english) | [ä¸­æ–‡](#chinese)

---

<a name="english"></a>

## ğŸ“– English Version

This directory contains three carefully selected papers that form a **complete technical evolution chain** in Vision-Language-Action (VLA) models for robotic manipulation.

### ğŸ“„ Paper Selection Rationale

These papers were chosen based on:
- âœ… **Recency**: Published in 2025-2026 (within 3-5 year requirement)
- âœ… **Quality**: Top-tier venues (AAAI Best Paper) and arXiv preprints
- âœ… **Relevance**: All address VLA for manipulation
- âœ… **Complementarity**: Each tackles a different aspect (language, space, vision)
- âœ… **Impact**: Collectively represent state-of-the-art progress

---

## ğŸ“‘ Paper Summaries

### 1ï¸âƒ£ RSS: Residual Semantic Steering (arXiv 2026)

**File**: [`RSS_Stable_Language_Guidance_2026.pdf`](./RSS_Stable_Language_Guidance_2026.pdf)

**Full Title**: *Stable Language Guidance for Vision-Language-Action Models*

**Authors**: Zhan et al.

**Key Contribution**: Addresses **modality collapse** where visual priors overwhelm linguistic signals

**Core Innovation**:
- **Monte Carlo Syntactic Integration (MCSI)**: LLM-driven distributional expansion
- **Residual Affordance Steering (RAS)**: Dual-stream decoding isolating language influence

**Results**: 82.2% success on LIBERO M8 (corrupted instructions)

**Gap**: âŒ Does not address visual attention mechanisms or temporal modeling

---

### 2ï¸âƒ£ SpatialVLA (arXiv 2025)

**File**: [`SpatialVLA_2025.pdf`](./SpatialVLA_2025.pdf)

**Full Title**: *SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model*

**Authors**: Qu, Song, Chen et al.

**Key Contribution**: Claims **spatial understanding is the keypoint** in robot manipulation

**Core Innovation**:
- **Ego3D Position Encoding**: Injects 3D spatial information
- **Adaptive Action Grids**: Dynamic spatial discretization

**Training**: 1.1M real-world episodes from OXE + RH20T

**Results**: 71.9% on SimplerEnv visual matching, 88.2% on spatial reasoning

**Gap**: âŒ Lacks temporal modeling, doesn't improve long-horizon performance

---

### 3ï¸âƒ£ â­ ReconVLA (AAAI 2026 Best Paper)

**File**: [`ReconVLA_AAAI2026.pdf`](./ReconVLA_AAAI2026.pdf)

**Full Title**: *ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver*

**Authors**: Song, Zhou, Zhao et al.

**Venue**: AAAI 2026 ğŸ† **Best Paper Award**

**Key Contribution**: First **implicit grounding** via gaze region reconstruction

**Core Innovation**:
- Diffusion transformer reconstructs target manipulation area
- Simulates human eye "gaze" mechanism
- End-to-end, no external models required

**Results**:
- 64.1% on CALVIN 5-task chains
- 95.6% on single tasks
- **31.5% degradation** from 1â†’5 tasks

**Critical Gap We Identified** âš ï¸:
- âŒ **Frame-independent reconstruction** â†’ no temporal coherence
- âŒ **Attention jumps** in sequential tasks
- âŒ This is the main focus of our TGM proposal

---

## ğŸ“Š Comparative Analysis

| Dimension | RSS | SpatialVLA | ReconVLA | **Our TGM** |
|-----------|-----|------------|----------|-------------|
| Language Robustness | âœ“âœ“ | âœ“ | âœ“ | âœ“ |
| Spatial Understanding | âœ— | âœ“âœ“ | âœ“ | âœ“ |
| Visual Grounding | âœ— | âœ“ | âœ“âœ“ | âœ“âœ“ |
| **Temporal Consistency** | âœ— | âœ— | âœ— | **âœ“âœ“** |

### Evolution Chain

```
RSS (Language) â†’ SpatialVLA (Space) â†’ ReconVLA (Vision) â†’ TGM (Temporal) â† Our Work
```

---

## ğŸ” Key Insights

1. **Convergence on Implicit Methods**: All recent works move from explicit (bounding boxes) to implicit representations

2. **Performance Paradox**: ReconVLA achieves 95.6% on single tasks but only 64.1% on 5-task chains

3. **Missing Dimension**: None address temporal attention stability across frames

---

## ğŸ“– Reading Guide

**Quick Overview (30 min)**:
1. ReconVLA Abstract + Figure 1
2. SpatialVLA Section 3 (Architecture)
3. RSS Table 1 (Results)

**Deep Dive (2-3 hours)**:
1. ReconVLA Section 4.3 (Ablation) + Figure 4 (Attention)
2. SpatialVLA Section 3.2 (Ego3D Encoding)
3. RSS Section 3.1 (MCSI Algorithm)

---

<a name="chinese"></a>

## ğŸ“– ä¸­æ–‡ç‰ˆæœ¬

æœ¬ç›®å½•åŒ…å«ä¸‰ç¯‡ç²¾å¿ƒæŒ‘é€‰çš„è®ºæ–‡ï¼Œå½¢æˆäº†æœºå™¨äººæ“ä½œVLAæ¨¡å‹çš„**å®Œæ•´æŠ€æœ¯æ¼”è¿›é“¾**ã€‚

### ğŸ“„ é€‰æ‹©ä¾æ®

- âœ… **æ—¶æ•ˆæ€§**: 2025-2026å¹´å‘è¡¨ï¼ˆç¬¦åˆ3-5å¹´è¦æ±‚ï¼‰
- âœ… **è´¨é‡**: é¡¶çº§ä¼šè®®ï¼ˆAAAIæœ€ä½³è®ºæ–‡ï¼‰å’ŒarXiv
- âœ… **ç›¸å…³æ€§**: å‡é’ˆå¯¹VLAæ“ä½œä»»åŠ¡
- âœ… **äº’è¡¥æ€§**: å„è‡ªè§£å†³ä¸åŒæ–¹é¢ï¼ˆè¯­è¨€ã€ç©ºé—´ã€è§†è§‰ï¼‰
- âœ… **å½±å“åŠ›**: ä»£è¡¨æœ€å…ˆè¿›è¿›å±•

---

## ğŸ“‘ è®ºæ–‡æ‘˜è¦

### 1ï¸âƒ£ RSS: æ®‹å·®è¯­ä¹‰å¼•å¯¼ï¼ˆarXiv 2026ï¼‰

**æ–‡ä»¶**: [`RSS_Stable_Language_Guidance_2026.pdf`](./RSS_Stable_Language_Guidance_2026.pdf)

**æ ‡é¢˜**: *VLAæ¨¡å‹çš„ç¨³å®šè¯­è¨€å¼•å¯¼*

**ä½œè€…**: Zhanç­‰

**æ ¸å¿ƒè´¡çŒ®**: è§£å†³**æ¨¡æ€åç¼©**ï¼ˆè§†è§‰å…ˆéªŒå‹å€’è¯­è¨€ä¿¡å·ï¼‰

**æ ¸å¿ƒåˆ›æ–°**:
- **è’™ç‰¹å¡æ´›å¥æ³•é›†æˆ**: LLMé©±åŠ¨çš„åˆ†å¸ƒå¼æ‰©å±•
- **æ®‹å·®èƒ½ä¾›æ€§å¼•å¯¼**: éš”ç¦»è¯­è¨€å½±å“çš„åŒæµè§£ç 

**ç»“æœ**: LIBERO M8ä¸Š82.2%æˆåŠŸç‡

**ç¼ºé™·**: âŒ æœªå…³æ³¨è§†è§‰æ³¨æ„åŠ›æˆ–æ—¶åºå»ºæ¨¡

---

### 2ï¸âƒ£ SpatialVLAï¼ˆarXiv 2025ï¼‰

**æ–‡ä»¶**: [`SpatialVLA_2025.pdf`](./SpatialVLA_2025.pdf)

**æ ‡é¢˜**: *æ¢ç´¢VLAæ¨¡å‹çš„ç©ºé—´è¡¨ç¤º*

**ä½œè€…**: Qu, Song, Chenç­‰

**æ ¸å¿ƒè´¡çŒ®**: å£°ç§°**ç©ºé—´ç†è§£æ˜¯å…³é”®**

**æ ¸å¿ƒåˆ›æ–°**:
- **Ego3Dä½ç½®ç¼–ç **: æ³¨å…¥3Dç©ºé—´ä¿¡æ¯
- **è‡ªé€‚åº”åŠ¨ä½œç½‘æ ¼**: åŠ¨æ€ç©ºé—´ç¦»æ•£åŒ–

**è®­ç»ƒ**: OXE + RH20Tçš„110ä¸‡çœŸå®ç‰‡æ®µ

**ç»“æœ**: SimplerEnv 71.9%ï¼Œç©ºé—´æ¨ç†88.2%

**ç¼ºé™·**: âŒ ç¼ºä¹æ—¶åºå»ºæ¨¡ï¼Œæœªæ”¹å–„é•¿æ—¶åºæ€§èƒ½

---

### 3ï¸âƒ£ â­ ReconVLAï¼ˆAAAI 2026æœ€ä½³è®ºæ–‡ï¼‰

**æ–‡ä»¶**: [`ReconVLA_AAAI2026.pdf`](./ReconVLA_AAAI2026.pdf)

**æ ‡é¢˜**: *é‡å»ºå¼VLAæ¨¡å‹ä½œä¸ºæœ‰æ•ˆæœºå™¨äººæ„ŸçŸ¥å™¨*

**ä½œè€…**: Song, Zhou, Zhaoç­‰

**ä¼šè®®**: AAAI 2026 ğŸ† **æœ€ä½³è®ºæ–‡å¥–**

**æ ¸å¿ƒè´¡çŒ®**: é¦–ä¸ªé€šè¿‡gazeåŒºåŸŸé‡å»ºçš„**éšå¼å®šä½**æ–¹æ³•

**æ ¸å¿ƒåˆ›æ–°**:
- æ‰©æ•£transformeré‡å»ºç›®æ ‡æ“ä½œåŒºåŸŸ
- æ¨¡æ‹Ÿäººçœ¼"gaze"æœºåˆ¶
- ç«¯åˆ°ç«¯ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹

**ç»“æœ**:
- CALVIN 5ä»»åŠ¡é“¾64.1%
- å•ä»»åŠ¡95.6%
- 1â†’5ä»»åŠ¡**ä¸‹é™31.5%**

**æˆ‘ä»¬è¯†åˆ«çš„å…³é”®ç¼ºé™·** âš ï¸:
- âŒ **é€å¸§ç‹¬ç«‹é‡å»º** â†’ æ— æ—¶åºè¿è´¯æ€§
- âŒ é¡ºåºä»»åŠ¡ä¸­**æ³¨æ„åŠ›è·³å˜**
- âŒ è¿™æ˜¯æˆ‘ä»¬TGMææ¡ˆçš„ä¸»è¦å…³æ³¨ç‚¹

---

## ğŸ“Š å¯¹æ¯”åˆ†æ

| ç»´åº¦ | RSS | SpatialVLA | ReconVLA | **æˆ‘ä»¬çš„TGM** |
|------|-----|------------|----------|--------------|
| è¯­è¨€é²æ£’æ€§ | âœ“âœ“ | âœ“ | âœ“ | âœ“ |
| ç©ºé—´ç†è§£ | âœ— | âœ“âœ“ | âœ“ | âœ“ |
| è§†è§‰å®šä½ | âœ— | âœ“ | âœ“âœ“ | âœ“âœ“ |
| **æ—¶åºä¸€è‡´æ€§** | âœ— | âœ— | âœ— | **âœ“âœ“** |

### æ¼”è¿›é“¾

```
RSS(è¯­è¨€) â†’ SpatialVLA(ç©ºé—´) â†’ ReconVLA(è§†è§‰) â†’ TGM(æ—¶åº) â† æˆ‘ä»¬çš„å·¥ä½œ
```

---

## ğŸ” å…³é”®æ´å¯Ÿ

1. **å‘éšå¼æ–¹æ³•æ”¶æ•›**: æ‰€æœ‰æœ€æ–°å·¥ä½œéƒ½ä»æ˜¾å¼ï¼ˆè¾¹ç•Œæ¡†ï¼‰è½¬å‘éšå¼è¡¨ç¤º

2. **æ€§èƒ½æ‚–è®º**: ReconVLAå•ä»»åŠ¡95.6%ä½†5ä»»åŠ¡é“¾ä»…64.1%

3. **ç¼ºå¤±ç»´åº¦**: æ— æ–¹æ³•å…³æ³¨è·¨å¸§æ—¶åºæ³¨æ„åŠ›ç¨³å®šæ€§

---

## ğŸ“– é˜…è¯»æŒ‡å—

**å¿«é€Ÿæ¦‚è§ˆï¼ˆ30åˆ†é’Ÿï¼‰**:
1. ReconVLAæ‘˜è¦ + å›¾1
2. SpatialVLAç¬¬3èŠ‚ï¼ˆæ¶æ„ï¼‰
3. RSSè¡¨1ï¼ˆç»“æœï¼‰

**æ·±å…¥ç ”è¯»ï¼ˆ2-3å°æ—¶ï¼‰**:
1. ReconVLAç¬¬4.3èŠ‚ï¼ˆæ¶ˆèï¼‰+ å›¾4ï¼ˆæ³¨æ„åŠ›ï¼‰
2. SpatialVLAç¬¬3.2èŠ‚ï¼ˆEgo3Dç¼–ç ï¼‰
3. RSSç¬¬3.1èŠ‚ï¼ˆMCSIç®—æ³•ï¼‰

---

## ğŸ“š Citation / å¼•ç”¨

```bibtex
@article{zhan2026stable,
  title={Stable Language Guidance for Vision-Language-Action Models},
  author={Zhan, Zhihao and others},
  journal={arXiv preprint arXiv:2601.04052},
  year={2026}
}

@article{qu2025spatialvla,
  title={SpatialVLA: Exploring Spatial Representations for VLA Model},
  author={Qu, Delin and Song, Haoming and others},
  journal={arXiv preprint arXiv:2501.15830},
  year={2025}
}

@inproceedings{song2026reconvla,
  title={ReconVLA: Reconstructive VLA Model as Effective Robot Perceiver},
  author={Song, Wenxuan and Zhou, Ziyang and others},
  booktitle={AAAI},
  note={Best Paper Award},
  year={2026}
}
```
